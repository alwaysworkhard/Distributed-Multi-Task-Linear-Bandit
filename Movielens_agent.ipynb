{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import NMF\n",
    "np.random.seed(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e30f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = 3\n",
    "d = d_ ** 2\n",
    "\n",
    "complete_rating = np.array(pd.read_csv('complete_ratings'))\n",
    "complete_rating = complete_rating[:100, 1:51] / 5\n",
    "\n",
    "model = NMF(n_components = d_, init = 'nndsvda', max_iter = 10000)\n",
    "W = model.fit_transform(complete_rating)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec860073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(W, H, d):\n",
    "    \n",
    "    phi_true = np.zeros((W.shape[0], H.shape[1], d))\n",
    "    psi_true = np.zeros((W.shape[0], H.shape[1], d))\n",
    "            \n",
    "    theta_true = np.eye(int(np.sqrt(d))).ravel()\n",
    "    theta_norm = np.linalg.norm(theta_true)\n",
    "    theta_true = theta_true / theta_norm\n",
    "    \n",
    "    for i in range(W.shape[0]):\n",
    "        \n",
    "        for j in range(H.shape[1]):\n",
    "            \n",
    "            temp = np.outer(W[i], H[:, j])\n",
    "            phi_true[i, j] = temp.ravel()\n",
    "            psi_true[i, j] = phi_true[i, j] + 1e-1 * np.random.normal(0, 1, d)\n",
    "            \n",
    "    max_value = np.max(np.dot(phi_true, theta_true))\n",
    "    phi_true = phi_true / max_value\n",
    "    psi_true = psi_true / max_value\n",
    "            \n",
    "    return phi_true, psi_true, theta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ed9016",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_true, psi_true, theta_true = generate_data(W, H, d)\n",
    "phi_true.shape, psi_true.shape, theta_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc576cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the beta\n",
    "def get_beta(rho, delta, V_bar, theta_true):\n",
    "    \n",
    "    ld = 1\n",
    "    beta = rho * np.sqrt(2 * np.log((np.sqrt(np.linalg.det(V_bar)) * (np.linalg.det(ld * np.eye(d)) ** (-1/2))) / delta)) + np.sqrt(ld) * np.linalg.norm(theta_true)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slsqp\n",
    "def get_decision(psi_action, theta_hat, V_bar, beta, d):\n",
    "    \n",
    "    def maximize_reward(theta, psi_a):\n",
    "\n",
    "        return -1.0 * psi_a.dot(theta)\n",
    "    \n",
    "    # constraint confidence set: || theta_hat - theta ||_V_bar < Beta\n",
    "    def constraint(theta, theta_hat, V_bar, beta):\n",
    "        \n",
    "        temp = np.array(theta_hat - theta.reshape(-1, 1))\n",
    "        norm = np.sqrt(temp.reshape(-1, 1).T.dot(V_bar).dot(temp.reshape(-1, 1)))\n",
    "        \n",
    "        return beta - norm[0][0]\n",
    "    \n",
    "    # constraints for optimization\n",
    "    beta_constraint = {'type': 'ineq', 'fun' : lambda theta: constraint(theta, theta_hat, V_bar, beta)}\n",
    "    \n",
    "    theta_list = []\n",
    "    reward_list = []\n",
    "    \n",
    "    for psi_a in psi_action:\n",
    "        \n",
    "        res = minimize(maximize_reward, x0 = np.ones(d), args = (psi_a), method = 'SLSQP', constraints = [beta_constraint], options = {'ftol': 1e-3, 'eps': 1e-10, 'maxiter': 1e6, 'disp': False})\n",
    "        theta_list.append(res.x)\n",
    "        reward_list.append(psi_a.dot(res.x))\n",
    "        # print(res.message)\n",
    "        \n",
    "    decision = np.argmax(reward_list)\n",
    "    theta_tilde = theta_list[decision]\n",
    "    \n",
    "    return decision, theta_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2701e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_plot_data(data_list, trials):\n",
    "    \n",
    "    result_lists = []\n",
    "    \n",
    "    for i in range(0, len(data_list), trials):\n",
    "        \n",
    "        subset = data_list[i : i + trials]\n",
    "        sum_array = np.sum([np.array(item[-1]) for item in subset], axis=0)\n",
    "        average_array = sum_array / trials\n",
    "        result_lists.append(average_array.tolist())\n",
    "    \n",
    "    return result_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387293f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 0.1\n",
    "ld = 1\n",
    "trials = 1\n",
    "alpha = 0.5\n",
    "num_agent = [3, 5]\n",
    "baseline_idx = 40\n",
    "delta_value = 1e-3\n",
    "iterations = 800000\n",
    "every_num_point = 25\n",
    "optimal_alg = []\n",
    "reward_alg = []\n",
    "baseline_alg = []\n",
    "cummulative_regret_alg = []\n",
    "cumulative_violate_alg = []\n",
    "cumulative_baseline_alg = []\n",
    "L = np.max(np.linalg.norm(phi_true, axis = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0af270",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_true_1 = phi_true.copy()\n",
    "phi_true_1[:, :, -1] = 0\n",
    "\n",
    "phi_true_2 = phi_true.copy()\n",
    "phi_true_2[:, :, 4] = 0\n",
    "\n",
    "phi_true_3 = phi_true.copy()\n",
    "phi_true_3[:, :, 0] = 0\n",
    "\n",
    "phi_true_group = np.array([phi_true, phi_true_1, phi_true_2, phi_true_3] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_true_1 = psi_true.copy()\n",
    "psi_true_1[:, :, -1] = 0\n",
    "\n",
    "psi_true_2 = psi_true.copy()\n",
    "psi_true_2[:, :, 4] = 0\n",
    "\n",
    "psi_true_3 = psi_true.copy()\n",
    "psi_true_3[:, :, 0] = 0\n",
    "\n",
    "psi_true_group = np.array([psi_true, psi_true_1, psi_true_2, psi_true_3] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_group = np.array([phi_true_group[i].dot(theta_true) for i in range(np.shape(phi_true_group)[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the index set for context\n",
    "sample_id = np.random.randint(0, np.shape(phi_true)[0], size=(trials, iterations))\n",
    "\n",
    "# generate the noise of reward for each iteration\n",
    "noise = np.random.normal(0, 1e-3, size = (num_agent[-1], trials, iterations))\n",
    "\n",
    "# calculate r_l and r_h\n",
    "product_results = np.dot(phi_true_group, theta_true)\n",
    "sorted_values = np.sort(product_results, axis=2)\n",
    "all_baseline_values = sorted_values[:, :, -baseline_idx]\n",
    "r_h = np.max(all_baseline_values)\n",
    "r_l = np.min(all_baseline_values)\n",
    "            \n",
    "# gererate rho_bar\n",
    "rho_bar = np.random.uniform(1e-10, alpha * r_l / (np.linalg.norm(theta_true) + r_h), size = (trials, iterations))\n",
    "\n",
    "# generate zeta\n",
    "zeta_data = np.random.normal(0, 1e-3, (trials, iterations, len(theta_true)))\n",
    "zeta_zero = zeta_data - np.mean(zeta_data, axis = 2, keepdims = True)\n",
    "zeta = zeta_zero / np.linalg.norm(zeta_zero, axis = 2, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our setting\n",
    "start = time.time()\n",
    "\n",
    "for M in num_agent:\n",
    "    \n",
    "    for T in range(trials):\n",
    "        \n",
    "        total_regret = 0\n",
    "        total_reward = 0\n",
    "        total_violate = 0\n",
    "        total_baseline = 0\n",
    "        cummulative_regret = []\n",
    "        cummulative_violate = []\n",
    "        cummulative_baseline = []\n",
    "        optimal_list = []\n",
    "        reward_list = []\n",
    "        baseline_list = []\n",
    "        \n",
    "        t_last = 0\n",
    "        V_last = ld * np.eye(d)\n",
    "        W_syn = np.zeros((d, d))\n",
    "        U_syn = np.zeros((d, 1))\n",
    "        \n",
    "        W_new_list = [np.zeros((d, d)) for i in range(M)]\n",
    "        U_new_list = [np.zeros((d, 1)) for i in range(M)]\n",
    "        \n",
    "        B = (iterations * np.log(M * iterations)) / (d * M)\n",
    "#         print('B is equal to', B)\n",
    "        \n",
    "        syn = 0\n",
    "\n",
    "        for t in range(1, iterations + 1): \n",
    "            \n",
    "#             print('')\n",
    "            print('Trial: {} t: {}:'.format(T,t))\n",
    "            \n",
    "            index = sample_id[T][t-1]\n",
    "            \n",
    "            for i in range(M):\n",
    "                \n",
    "                phi = phi_true_group[i][index]\n",
    "                psi = psi_true_group[i][index]\n",
    "                original = rewards_group[i][index]\n",
    "\n",
    "                x_star = np.argmax(np.dot(np.array(psi), theta_true))\n",
    "                optimal = original[x_star]\n",
    "#                 print('best decision is:', np.argmax(original))\n",
    "                \n",
    "                x_b = np.argsort(original)[::-1][baseline_idx]\n",
    "                r_b = original[x_b]\n",
    "#                 print('baseline decision is:', x_b)\n",
    "                \n",
    "                V_bar = ld * np.eye(d) + W_syn + W_new_list[i]\n",
    "                theta_hat = np.dot(np.linalg.inv(V_bar), (U_syn + U_new_list[i]))\n",
    "\n",
    "                # construct the confidence ellipsoid beta\n",
    "                beta = get_beta(rho = np.sqrt(1 + R ** 2), delta = delta_value / 2, V_bar = V_bar, theta_true = theta_true)\n",
    "                \n",
    "                #construct the trimmed action set\n",
    "                tas = (psi.dot(theta_hat) >= beta * L / np.sqrt(np.min(np.linalg.eigvals(V_bar))) + (1 - alpha) * r_b)\n",
    "                phi_set = phi[tas.ravel()]\n",
    "                psi_set = psi[tas.ravel()]\n",
    "                original_set = original[tas.ravel()]\n",
    "                \n",
    "                # get the best action\n",
    "                if (psi_set.size != 0) and (np.min(np.linalg.eigvals(V_bar)) >= np.square(2 * L * beta / ((optimal - r_h) + alpha * r_b))):\n",
    "                \n",
    "                    decision, theta_tilde = get_decision(psi_set, theta_hat, V_bar, beta, d)\n",
    "                    psi_new = psi_set[decision]\n",
    "                    y = original_set[decision]\n",
    "#                     print(\"play learner's decision:\", decision)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    decision = x_b\n",
    "                    total_baseline += 1\n",
    "                    psi_new = (1 - rho_bar[T][t-1]) * psi[decision] + rho_bar[T][t-1] * zeta[T][t-1]\n",
    "                    y = (1 - rho_bar[T][t-1]) * original[decision] + rho_bar[T][t-1] * np.dot(zeta[T][t-1], theta_true)\n",
    "#                     print(\"play conservative decision:\", decision)\n",
    "                    \n",
    "                regret = optimal - y\n",
    "                total_regret = total_regret + regret\n",
    "                total_reward = total_reward + y\n",
    "\n",
    "                # update W_new and U_new\n",
    "                W_new_list[i] = W_new_list[i] + np.outer(psi_new, psi_new)\n",
    "                U_new_list[i] = U_new_list[i] + psi_new.reshape(-1, 1) * (y + noise[i][T][t-1])\n",
    "                V = ld * np.eye(d) + W_syn + W_new_list[i]\n",
    "                \n",
    "                LHS_condition = np.log(np.linalg.det(V) / np.linalg.det(V_last)) * (t - t_last)\n",
    "                \n",
    "                if LHS_condition >= B:\n",
    "                    \n",
    "#                     print('synchronization start for agent', i)\n",
    "#                     print('LHS condition is:', LHS_condition)\n",
    "                    \n",
    "                    syn = 1\n",
    "                    \n",
    "#                 print('----------')\n",
    "                \n",
    "                if y < (1 - alpha) * r_b:\n",
    "                    \n",
    "                    total_violate += 1\n",
    "#                     print('violate the constraint')\n",
    "                \n",
    "            if syn == 1:\n",
    "                \n",
    "                W_syn = W_syn + np.sum(W_new_list, axis=0)\n",
    "                U_syn = U_syn + np.sum(U_new_list, axis=0)\n",
    "                \n",
    "                W_new_list = [np.zeros((d, d)) for i in range(M)]\n",
    "                U_new_list = [np.zeros((d, 1)) for i in range(M)]\n",
    "                t_last = t\n",
    "                V_last = ld * np.eye(d) + W_syn\n",
    "                \n",
    "                syn = 0\n",
    "                \n",
    "#             print('cummulative_regret is: ', total_regret)\n",
    "            cummulative_regret.append(total_regret)\n",
    "            cummulative_violate.append(total_violate)\n",
    "            cummulative_baseline.append(total_baseline)\n",
    "            optimal_list.append(np.max(np.dot(phi, theta_true)))\n",
    "            reward_list.append(y)\n",
    "            baseline_list.append((1 - alpha) * r_b)\n",
    "            \n",
    "        cummulative_regret_alg.append((M, T, cummulative_regret))\n",
    "        cumulative_violate_alg.append((M, T, cummulative_violate))\n",
    "        cumulative_baseline_alg.append((M, T, cummulative_baseline))\n",
    "        optimal_alg.append((M, T, optimal_list))\n",
    "        reward_alg.append((M, T, reward_list))\n",
    "        baseline_alg.append((M, T, baseline_list))\n",
    "        \n",
    "end = time.time()\n",
    "print('Finished! The total time we use is: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a922d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lists = prepare_plot_data(cummulative_regret_alg, trials)\n",
    "x_value = np.array([i for i in range(len(plot_lists[0]))])\n",
    "y_1 = [plot_lists[0][i] / num_agent[0] for i in range(len(plot_lists[0]))]\n",
    "y_2 = [plot_lists[1][i] / num_agent[1] for i in range(len(plot_lists[1]))]\n",
    "\n",
    "plt.figure(figsize=(12,8), dpi=300)\n",
    "\n",
    "colors = (['black', 'blue', 'darkgreen', 'purple', 'darkred', 'grey'])\n",
    "markers = ['*', 's', 'o', 'X', '^', 'P']\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 25\n",
    "plt.rcParams['ytick.labelsize'] = 25\n",
    "plt.rc('legend', fontsize = 25)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(style='sci', axis='both', scilimits=(0, 0), useOffset=False)\n",
    "\n",
    "plt.plot(x_value, y_1, label = 'M = 3', color = colors[1], linewidth=3)\n",
    "plt.scatter(x_value[::50000], y_1[::50000], label = 'M = 3', marker = markers[1], color = colors[1], s=300)\n",
    "plt.plot(x_value, y_2, label = 'M = 5', color = colors[2], linewidth=3)\n",
    "plt.scatter(x_value[::50000], y_2[::50000], label = 'M = 5', marker = markers[2], color = colors[2], s=300)\n",
    "\n",
    "legend_elements = [mlines.Line2D([0], [0], color=colors[1], lw = 5, label = 'M = 3', marker = markers[1], markersize = 15), \n",
    "                   mlines.Line2D([0], [0], color=colors[2], lw = 5, label = 'M = 5', marker = markers[2], markersize = 15)]\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('round,t', fontsize=25)\n",
    "plt.ylabel('per-agent cumulative regret Rt', fontsize=25)\n",
    "plt.title('movielens data', fontsize=25)\n",
    "plt.legend(handles=legend_elements)\n",
    "# plt.savefig('plot_8.pdf', dpi=600, bbox_inches = 'tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76298876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
