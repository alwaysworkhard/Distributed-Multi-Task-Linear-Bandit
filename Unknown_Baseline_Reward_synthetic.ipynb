{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import NMF\n",
    "np.random.seed(9000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63782928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_context):\n",
    "    \n",
    "    d = 2\n",
    "    num_action = 90\n",
    "    \n",
    "    theta_true = np.array([1, 1])\n",
    "    \n",
    "    actions_per_group = num_action // 9\n",
    "    phi_true = np.zeros((num_context, num_action, d))\n",
    "\n",
    "    for i in range(num_context):\n",
    "        \n",
    "        for group in range(9):\n",
    "            \n",
    "            lower_bound = 0.8 - group * 0.1\n",
    "            upper_bound = 1.0 - group * 0.1\n",
    "            \n",
    "            for j in range(actions_per_group):\n",
    "                \n",
    "                while True:\n",
    "                    \n",
    "                    vec = np.random.randn(d)\n",
    "                    \n",
    "                    if lower_bound <= np.dot(vec, theta_true) <= upper_bound and all(vec >= 0):\n",
    "                        \n",
    "                        phi_true[i][group * actions_per_group + j] = vec\n",
    "                        \n",
    "                        break\n",
    "                        \n",
    "    psi_true = np.zeros((num_context, num_action, d))\n",
    "    \n",
    "    for i in range(num_context):\n",
    "\n",
    "        for a in range(num_action):\n",
    "\n",
    "            psi_true[i, a] = phi_true[i, a] + 1e-1 * np.random.normal(0, 1, d)\n",
    "            \n",
    "    rewards = phi_true.dot(theta_true)\n",
    "\n",
    "    return phi_true, psi_true, theta_true, rewards, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc576cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the beta\n",
    "def get_beta(rho, delta, V_bar, theta_true):\n",
    "    \n",
    "    ld = 0.1\n",
    "    beta = rho * np.sqrt(2 * np.log((np.sqrt(np.linalg.det(V_bar)) * (np.linalg.det(ld * np.eye(d)) ** (-1/2))) / delta)) + np.sqrt(ld) * np.linalg.norm(theta_true)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slsqp\n",
    "def get_decision(psi_action, theta_hat, V_bar, beta, d):\n",
    "    \n",
    "    def maximize_reward(theta, psi_a):\n",
    "\n",
    "        return -1.0 * psi_a.dot(theta)\n",
    "    \n",
    "    # constraint confidence set: || theta_hat - theta ||_V_bar < Beta\n",
    "    def constraint(theta, theta_hat, V_bar, beta):\n",
    "        \n",
    "        temp = np.array(theta_hat - theta.reshape(-1, 1))\n",
    "        norm = np.sqrt(temp.reshape(-1, 1).T.dot(V_bar).dot(temp.reshape(-1, 1)))\n",
    "        \n",
    "        return beta - norm[0][0]\n",
    "    \n",
    "    # constraints for optimization\n",
    "    beta_constraint = {'type': 'ineq', 'fun' : lambda theta: constraint(theta, theta_hat, V_bar, beta)}\n",
    "    \n",
    "    theta_list = []\n",
    "    reward_list = []\n",
    "    \n",
    "    for psi_a in psi_action:\n",
    "        \n",
    "        res = minimize(maximize_reward, x0 = np.ones(d), args = (psi_a), method = 'SLSQP', constraints = [beta_constraint], options = {'ftol': 1e-3, 'eps': 1e-10, 'maxiter': 1e6, 'disp': False})\n",
    "        theta_list.append(res.x)\n",
    "        reward_list.append(psi_a.dot(res.x))\n",
    "        \n",
    "    decision = np.argmax(reward_list)\n",
    "    theta_tilde = theta_list[decision]\n",
    "    \n",
    "    return decision, theta_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8fe0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slsqp\n",
    "def get_decision_(psi_b, theta_hat, V_bar, beta, d):\n",
    "    \n",
    "    def maximize_reward(theta, psi_b):\n",
    "\n",
    "        return -1.0 * psi_b.dot(theta)\n",
    "    \n",
    "    # constraint confidence set: || theta_hat - theta ||_V_bar < Beta\n",
    "    def constraint(theta, theta_hat, V_bar, beta):\n",
    "        \n",
    "        temp = np.array(theta_hat - theta.reshape(-1, 1))\n",
    "        norm = np.sqrt(temp.reshape(-1, 1).T.dot(V_bar).dot(temp.reshape(-1, 1)))\n",
    "        \n",
    "        return beta - norm[0][0]\n",
    "    \n",
    "    # constraints for optimization\n",
    "    beta_constraint = {'type': 'ineq', 'fun' : lambda theta: constraint(theta, theta_hat, V_bar, beta)}\n",
    "    \n",
    "    res = minimize(maximize_reward, x0 = np.ones(d), args = (psi_b), method = 'SLSQP', constraints = [beta_constraint], options = {'ftol': 1e-3, 'eps': 1e-10, 'maxiter': 1e6, 'disp': False})\n",
    "    value = psi_b.dot(res.x)\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2701e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_plot_data(data_list, trials, iterations, num_agent, every_num_point):\n",
    "    \n",
    "    new_list = []\n",
    "\n",
    "    for T in range(trials):\n",
    "\n",
    "        new_list.append([item[1] for item in data_list if item[0] == T])\n",
    "\n",
    "    new_list = np.array(new_list).reshape(trials, iterations).tolist()\n",
    "    \n",
    "    data_value = np.zeros(iterations)\n",
    "    \n",
    "    for T in range(trials):\n",
    "        \n",
    "        data_value += np.array(new_list[T])\n",
    "        \n",
    "    data_value = data_value / trials\n",
    "    \n",
    "    x_value = [0]\n",
    "    y_value = [data_value[0] / num_agent[0]]\n",
    "    \n",
    "    for num in range(int(iterations / every_num_point)):\n",
    "        \n",
    "        x_value.append((num + 1) * every_num_point - 1)\n",
    "        y_value.append(data_value[(num + 1) * every_num_point - 1] / num_agent[0])\n",
    "    \n",
    "    return x_value, y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387293f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "R = 0.1\n",
    "ld = 0.1\n",
    "trials = 50\n",
    "alpha = 0.5\n",
    "num_agent = [1]\n",
    "baseline_idx = 80\n",
    "delta_value = 1e-3\n",
    "iterations = 50000\n",
    "every_num_point = 25\n",
    "optimal_alg_unknown = []\n",
    "reward_alg_unknown = []\n",
    "baseline_alg_unknown = []\n",
    "cummulative_regret_alg_unknown = []\n",
    "cumulative_violate_alg_unknown = []\n",
    "cumulative_baseline_alg_unknown = []\n",
    "optimal_alg = []\n",
    "reward_alg = []\n",
    "baseline_alg = []\n",
    "cummulative_regret_alg = []\n",
    "cumulative_violate_alg = []\n",
    "cumulative_baseline_alg = []\n",
    "theta_true = np.array([1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_true_group_T = []\n",
    "psi_true_group_T = []\n",
    "rewards_group_T = []\n",
    "sample_id_T = []\n",
    "noise_T = []\n",
    "r_h_T = []\n",
    "r_l_T = []\n",
    "rho_bar_T = []\n",
    "zeta_T = []\n",
    "\n",
    "for T in range(trials):\n",
    "    \n",
    "    phi_true, psi_true, theta_true, rewards, d = generate_data(int(1e2))\n",
    "    L = np.max(np.linalg.norm(phi_true, axis = 2))\n",
    "    \n",
    "    phi_true_1 = phi_true.copy()\n",
    "    phi_true_1[:, :, 1] = 0\n",
    "\n",
    "    phi_true_2 = phi_true.copy()\n",
    "    phi_true_2[:, :, 0] = 0\n",
    "    \n",
    "    psi_true_1 = psi_true.copy()\n",
    "    psi_true_1[:, :, 1] = 0\n",
    "\n",
    "    psi_true_2 = psi_true.copy()\n",
    "    psi_true_2[:, :, 0] = 0\n",
    "    \n",
    "    phi_true_group = np.array([phi_true, phi_true_1, phi_true_2] * 3)\n",
    "    psi_true_group = np.array([psi_true, psi_true_1, psi_true_2] * 3)\n",
    "    \n",
    "    rewards_group = np.array([phi_true_group[i].dot(theta_true) for i in range(np.shape(phi_true_group)[0])])\n",
    "    \n",
    "    # generate the index set for context\n",
    "    sample_id = np.random.randint(0, np.shape(phi_true)[0], size = iterations)\n",
    "\n",
    "    # generate the noise of reward for each iteration\n",
    "    noise = np.random.normal(0, 1e-3, size = (num_agent[-1], trials, iterations))\n",
    "\n",
    "    # calculate r_l and r_h\n",
    "    product_results = np.dot(phi_true_group, theta_true)\n",
    "    sorted_values = np.sort(product_results, axis=2)\n",
    "    all_baseline_values = sorted_values[:, :, -baseline_idx]\n",
    "    r_h = np.max(all_baseline_values)\n",
    "    r_l = np.min(all_baseline_values)\n",
    "\n",
    "    # gererate rho_bar\n",
    "    rho_bar = np.random.uniform(1e-10, np.min(alpha) * r_l / (np.linalg.norm(theta_true) + r_h), size = (trials, iterations))\n",
    "\n",
    "    # generate zeta\n",
    "    zeta_data = np.random.normal(0, 1e-3, (trials, iterations, len(theta_true)))\n",
    "    zeta_zero = zeta_data - np.mean(zeta_data, axis = 2, keepdims = True)\n",
    "    zeta = zeta_zero / np.linalg.norm(zeta_zero, axis = 2, keepdims = True)\n",
    "    \n",
    "    phi_true_group_T.append(phi_true_group)\n",
    "    psi_true_group_T.append(psi_true_group)\n",
    "    rewards_group_T.append(rewards_group)\n",
    "    sample_id_T.append(sample_id)\n",
    "    noise_T.append(noise)\n",
    "    r_h_T.append(r_h)\n",
    "    r_l_T.append(r_l)\n",
    "    rho_bar_T.append(rho_bar)\n",
    "    zeta_T.append(zeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445fb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unknown Baseline Reward setting\n",
    "start = time.time()\n",
    "\n",
    "for M in num_agent:\n",
    "    \n",
    "    for T in range(trials):\n",
    "        \n",
    "        phi_true_group = phi_true_group_T[T]\n",
    "        psi_true_group = psi_true_group_T[T]\n",
    "        rewards_group = rewards_group_T[T]\n",
    "        sample_id = sample_id_T[T]\n",
    "        noise = noise_T[T]\n",
    "        r_h = r_h_T[T]\n",
    "        r_l = r_l_T[T]\n",
    "        rho_bar = rho_bar_T[T]\n",
    "        zeta = zeta_T[T]\n",
    "        L = np.max(np.linalg.norm(phi_true_group[0], axis = 2))\n",
    "        \n",
    "        total_regret = 0\n",
    "        total_reward = 0\n",
    "        total_violate = 0\n",
    "        total_baseline = 0\n",
    "        cummulative_regret = []\n",
    "        cummulative_violate = []\n",
    "        cummulative_baseline = []\n",
    "        optimal_list = []\n",
    "        reward_list = []\n",
    "        baseline_list = []\n",
    "        \n",
    "        t_last = 0\n",
    "        V_last = ld * np.eye(d)\n",
    "        W_syn = np.zeros((d, d))\n",
    "        U_syn = np.zeros((d, 1))\n",
    "        \n",
    "        W_new_list = [np.zeros((d, d)) for i in range(M)]\n",
    "        U_new_list = [np.zeros((d, 1)) for i in range(M)]\n",
    "        \n",
    "        B = (iterations * np.log(M * iterations)) / (d * M)\n",
    "        \n",
    "        syn = 0\n",
    "\n",
    "        for t in range(1, iterations + 1): \n",
    "            \n",
    "            index = sample_id[t-1]\n",
    "            \n",
    "            for i in range(M):\n",
    "                \n",
    "                phi = phi_true_group[i][index]\n",
    "                psi = psi_true_group[i][index]\n",
    "                original = rewards_group[i][index]\n",
    "\n",
    "                x_star = np.argmax(np.dot(np.array(psi), theta_true))\n",
    "                optimal = original[x_star]\n",
    "                \n",
    "                x_b = np.argsort(original)[::-1][baseline_idx]\n",
    "                r_b = original[x_b]\n",
    "                \n",
    "                V_bar = ld * np.eye(d) + W_syn + W_new_list[i]\n",
    "                theta_hat = np.dot(np.linalg.inv(V_bar), (U_syn + U_new_list[i]))\n",
    "\n",
    "                # construct the confidence ellipsoid beta\n",
    "                beta = get_beta(rho = np.sqrt(1 + R ** 2), delta = delta_value / 2, V_bar = V_bar, theta_true = theta_true)\n",
    "                \n",
    "                #construct the trimmed action set\n",
    "                value = get_decision_(psi[x_b], theta_hat, V_bar, beta, d)\n",
    "                tas = (psi.dot(theta_hat) >= beta * L / np.sqrt(np.min(np.linalg.eigvals(V_bar))) + (1 - alpha) * value)\n",
    "                phi_set = phi[tas.ravel()]\n",
    "                psi_set = psi[tas.ravel()]\n",
    "                original_set = original[tas.ravel()]\n",
    "                \n",
    "                # get the best action\n",
    "                if (psi_set.size != 0) and (np.min(np.linalg.eigvals(V_bar)) >= np.square(2 * (2 - alpha) * L * beta / ((optimal - r_h) + alpha * r_l))):\n",
    "                \n",
    "                    decision, theta_tilde = get_decision(psi_set, theta_hat, V_bar, beta, d)\n",
    "                    psi_new = psi_set[decision]\n",
    "                    y = original_set[decision]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    decision = x_b\n",
    "                    total_baseline += 1\n",
    "                    psi_new = (1 - rho_bar[T][t-1]) * psi[decision] + rho_bar[T][t-1] * zeta[T][t-1]\n",
    "                    y = (1 - rho_bar[T][t-1]) * original[decision] + rho_bar[T][t-1] * np.dot(zeta[T][t-1], theta_true)\n",
    "                    \n",
    "                regret = optimal - y\n",
    "                total_regret = total_regret + regret\n",
    "                total_reward = total_reward + y\n",
    "\n",
    "                # update W_new and U_new\n",
    "                W_new_list[i] = W_new_list[i] + np.outer(psi_new, psi_new)\n",
    "                U_new_list[i] = U_new_list[i] + psi_new.reshape(-1, 1) * (y + noise[i][T][t-1])\n",
    "                V = ld * np.eye(d) + W_syn + W_new_list[i]\n",
    "                \n",
    "                LHS_condition = np.log(np.linalg.det(V) / np.linalg.det(V_last)) * (t - t_last)\n",
    "                \n",
    "                if LHS_condition >= B:\n",
    "                    \n",
    "                    syn = 1\n",
    "                    \n",
    "                if y < (1 - alpha) * r_b:\n",
    "                    \n",
    "                    total_violate += 1\n",
    "                    \n",
    "            if syn == 1:\n",
    "                \n",
    "                W_syn = W_syn + np.sum(W_new_list, axis=0)\n",
    "                U_syn = U_syn + np.sum(U_new_list, axis=0)\n",
    "                \n",
    "                W_new_list = [np.zeros((d, d)) for i in range(M)]\n",
    "                U_new_list = [np.zeros((d, 1)) for i in range(M)]\n",
    "                t_last = t\n",
    "                V_last = ld * np.eye(d) + W_syn\n",
    "                \n",
    "                syn = 0\n",
    "                \n",
    "            cummulative_regret.append(total_regret)\n",
    "            cummulative_violate.append(total_violate)\n",
    "            cummulative_baseline.append(total_baseline)\n",
    "            optimal_list.append(np.max(np.dot(phi, theta_true)))\n",
    "            reward_list.append(y)\n",
    "            baseline_list.append((1 - alpha) * r_b)\n",
    "            \n",
    "        cummulative_regret_alg_unknown.append((T, cummulative_regret))\n",
    "        cumulative_violate_alg_unknown.append((T, cummulative_violate))\n",
    "        cumulative_baseline_alg_unknown.append((T, cummulative_baseline))\n",
    "        optimal_alg_unknown.append((T, optimal_list))\n",
    "        reward_alg_unknown.append((T, reward_list))\n",
    "        baseline_alg_unknown.append((T, baseline_list))\n",
    "        \n",
    "end = time.time()\n",
    "print('Finished! The total time we use is: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our setting\n",
    "start = time.time()\n",
    "\n",
    "for M in num_agent:\n",
    "    \n",
    "    for T in range(trials):\n",
    "        \n",
    "        phi_true_group = phi_true_group_T[T]\n",
    "        psi_true_group = psi_true_group_T[T]\n",
    "        rewards_group = rewards_group_T[T]\n",
    "        sample_id = sample_id_T[T]\n",
    "        noise = noise_T[T]\n",
    "        r_h = r_h_T[T]\n",
    "        r_l = r_l_T[T]\n",
    "        rho_bar = rho_bar_T[T]\n",
    "        zeta = zeta_T[T]\n",
    "        L = np.max(np.linalg.norm(phi_true_group[0], axis = 2))\n",
    "        \n",
    "        total_regret = 0\n",
    "        total_reward = 0\n",
    "        total_violate = 0\n",
    "        total_baseline = 0\n",
    "        cummulative_regret = []\n",
    "        cummulative_violate = []\n",
    "        cummulative_baseline = []\n",
    "        optimal_list = []\n",
    "        reward_list = []\n",
    "        baseline_list = []\n",
    "        \n",
    "        t_last = 0\n",
    "        V_last = ld * np.eye(d)\n",
    "        W_syn = np.zeros((d, d))\n",
    "        U_syn = np.zeros((d, 1))\n",
    "        \n",
    "        W_new_list = [np.zeros((d, d)) for i in range(M)]\n",
    "        U_new_list = [np.zeros((d, 1)) for i in range(M)]\n",
    "        \n",
    "        B = (iterations * np.log(M * iterations)) / (d * M)\n",
    "        \n",
    "        syn = 0\n",
    "\n",
    "        for t in range(1, iterations + 1): \n",
    "            \n",
    "            index = sample_id[t-1]\n",
    "            \n",
    "            for i in range(M):\n",
    "                \n",
    "                phi = phi_true_group[i][index]\n",
    "                psi = psi_true_group[i][index]\n",
    "                original = rewards_group[i][index]\n",
    "\n",
    "                x_star = np.argmax(np.dot(np.array(psi), theta_true))\n",
    "                optimal = original[x_star]\n",
    "                \n",
    "                x_b = np.argsort(original)[::-1][baseline_idx]\n",
    "                r_b = original[x_b]\n",
    "                \n",
    "                V_bar = ld * np.eye(d) + W_syn + W_new_list[i]\n",
    "                theta_hat = np.dot(np.linalg.inv(V_bar), (U_syn + U_new_list[i]))\n",
    "\n",
    "                # construct the confidence ellipsoid beta\n",
    "                beta = get_beta(rho = np.sqrt(1 + R ** 2), delta = delta_value / 2, V_bar = V_bar, theta_true = theta_true)\n",
    "                \n",
    "                #construct the trimmed action set\n",
    "                tas = (psi.dot(theta_hat) >= beta * L / np.sqrt(np.min(np.linalg.eigvals(V_bar))) + (1 - alpha) * r_b)\n",
    "                phi_set = phi[tas.ravel()]\n",
    "                psi_set = psi[tas.ravel()]\n",
    "                original_set = original[tas.ravel()]\n",
    "                \n",
    "                # get the best action\n",
    "                if (psi_set.size != 0) and (np.min(np.linalg.eigvals(V_bar)) >= np.square(2 * L * beta / ((optimal - r_h) + alpha * r_b))):\n",
    "                \n",
    "                    decision, theta_tilde = get_decision(psi_set, theta_hat, V_bar, beta, d)\n",
    "                    psi_new = psi_set[decision]\n",
    "                    y = original_set[decision]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    decision = x_b\n",
    "                    total_baseline += 1\n",
    "                    psi_new = (1 - rho_bar[T][t-1]) * psi[decision] + rho_bar[T][t-1] * zeta[T][t-1]\n",
    "                    y = (1 - rho_bar[T][t-1]) * original[decision] + rho_bar[T][t-1] * np.dot(zeta[T][t-1], theta_true)\n",
    "                    \n",
    "                regret = optimal - y\n",
    "                total_regret = total_regret + regret\n",
    "                total_reward = total_reward + y\n",
    "\n",
    "                # update W_new and U_new\n",
    "                W_new_list[i] = W_new_list[i] + np.outer(psi_new, psi_new)\n",
    "                U_new_list[i] = U_new_list[i] + psi_new.reshape(-1, 1) * (y + noise[i][T][t-1])\n",
    "                V = ld * np.eye(d) + W_syn + W_new_list[i]\n",
    "                \n",
    "                LHS_condition = np.log(np.linalg.det(V) / np.linalg.det(V_last)) * (t - t_last)\n",
    "                \n",
    "                if LHS_condition >= B:\n",
    "                    \n",
    "                    syn = 1\n",
    "                    \n",
    "                if y < (1 - alpha) * r_b:\n",
    "                    \n",
    "                    total_violate += 1\n",
    "                    \n",
    "            if syn == 1:\n",
    "                \n",
    "                W_syn = W_syn + np.sum(W_new_list, axis=0)\n",
    "                U_syn = U_syn + np.sum(U_new_list, axis=0)\n",
    "                \n",
    "                W_new_list = [np.zeros((d, d)) for i in range(M)]\n",
    "                U_new_list = [np.zeros((d, 1)) for i in range(M)]\n",
    "                t_last = t\n",
    "                V_last = ld * np.eye(d) + W_syn\n",
    "                \n",
    "                syn = 0\n",
    "                \n",
    "            cummulative_regret.append(total_regret)\n",
    "            cummulative_violate.append(total_violate)\n",
    "            cummulative_baseline.append(total_baseline)\n",
    "            optimal_list.append(np.max(np.dot(phi, theta_true)))\n",
    "            reward_list.append(y)\n",
    "            baseline_list.append((1 - alpha) * r_b)\n",
    "            \n",
    "        cummulative_regret_alg.append((T, cummulative_regret))\n",
    "        cumulative_violate_alg.append((T, cummulative_violate))\n",
    "        cumulative_baseline_alg.append((T, cummulative_baseline))\n",
    "        optimal_alg.append((T, optimal_list))\n",
    "        reward_alg.append((T, reward_list))\n",
    "        baseline_alg.append((T, baseline_list))\n",
    "        \n",
    "end = time.time()\n",
    "print('Finished! The total time we use is: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_value, y_alg_unknown = prepare_plot_data(cummulative_regret_alg_unknown, trials, iterations, num_agent, every_num_point)\n",
    "x_value_r, y_optimal_alg_unknown = prepare_plot_data(optimal_alg_unknown, trials, iterations, num_agent, 1)\n",
    "x_value_r, y_reward_alg_unknown = prepare_plot_data(reward_alg_unknown, trials, iterations, num_agent, 1)\n",
    "x_value_r, y_baseline_alg_unknown = prepare_plot_data(baseline_alg_unknown, trials, iterations, num_agent, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the figure\n",
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "colors = (['black', 'blue', 'darkgreen', 'purple', 'darkred', 'grey'])\n",
    "markers = ['*', 's', 'o', 'X', '^', 'P']\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 25\n",
    "plt.rcParams['ytick.labelsize'] = 25\n",
    "plt.rc('legend', fontsize = 25)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(style='sci', axis='both', scilimits=(0, 0), useOffset=False)\n",
    "\n",
    "plt.plot(x_value, y_alg, label = 'DiSC-UCB', color = colors[1], linewidth=3)\n",
    "plt.scatter(x_value[::500], y_alg[::500], label = 'DiSC-UCB', marker = markers[1], color = colors[1], s=300)\n",
    "plt.plot(x_value, y_alg_unknown, label = 'DiSC-UCB2', color = colors[2], linewidth=3)\n",
    "plt.scatter(x_value[::500], y_alg_unknown[::500], label = 'DiSC-UCB2', marker = markers[2], color = colors[2], s=300)\n",
    "\n",
    "legend_elements = [mlines.Line2D([0], [0], color=colors[1], lw = 5, label = 'DiSC-UCB', marker = markers[1], markersize = 15), \n",
    "                   mlines.Line2D([0], [0], color=colors[2], lw = 5, label = 'DiSC-UCB2', marker = markers[2], markersize = 15)]\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('round t', fontsize=25)\n",
    "plt.ylabel('cumulative regret Rt', fontsize=25)\n",
    "plt.title('synthetic data', fontsize=25)\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ced6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_plot_data_T(data_list, trials, iterations, num_agent, every_num_point):\n",
    "    \n",
    "    new_list = []\n",
    "\n",
    "    for T in range(trials):\n",
    "\n",
    "        new_list.append([item[1] for item in data_list if item[0] == T])\n",
    "\n",
    "    new_list = np.array(new_list).reshape(trials, iterations).tolist()\n",
    "    \n",
    "    data_value = np.zeros(iterations)\n",
    "    \n",
    "    for T in range(trials):\n",
    "        \n",
    "        data_value += np.array(new_list[T])\n",
    "        \n",
    "    data_value = data_value / trials\n",
    "    \n",
    "    x_value = [0]\n",
    "    y_value = [data_value[0] / num_agent[0]]\n",
    "    \n",
    "    for num in range(int(iterations / every_num_point)):\n",
    "        \n",
    "        x_value.append((num + 1) * every_num_point - 1)\n",
    "        y_value.append(data_value[(num + 1) * every_num_point - 1] / (num_agent[0] * (num + 1) * every_num_point))\n",
    "    \n",
    "    return x_value, y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_value, y_alg = prepare_plot_data_T(cummulative_regret_alg, trials, iterations, num_agent, every_num_point)\n",
    "x_value, y_alg_unknown = prepare_plot_data_T(cummulative_regret_alg_unknown, trials, iterations, num_agent, every_num_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the figure\n",
    "plt.figure(figsize=(12, 8), dpi=300)\n",
    "\n",
    "colors = (['black', 'blue', 'darkgreen', 'purple', 'darkred', 'grey'])\n",
    "markers = ['*', 's', 'o', 'X', '^', 'P']\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = 25\n",
    "plt.rcParams['ytick.labelsize'] = 25\n",
    "plt.rc('legend', fontsize = 25)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.ticklabel_format(style='sci', axis='both', scilimits=(0, 0), useOffset=False)\n",
    "\n",
    "plt.plot(x_value, y_alg, label = 'DiSC-UCB', color = colors[1], linewidth=3)\n",
    "plt.scatter(x_value[::5000], y_alg[::5000], label = 'DiSC-UCB', marker = markers[1], color = colors[1], s=300)\n",
    "plt.plot(x_value, y_alg_unknown, label = 'DiSC-UCB2', color = colors[2], linewidth=3)\n",
    "plt.scatter(x_value[::5000], y_alg_unknown[::5000], label = 'DiSC-UCB2', marker = markers[2], color = colors[2], s=300)\n",
    "\n",
    "legend_elements = [mlines.Line2D([0], [0], color=colors[1], lw = 5, label = 'DiSC-UCB', marker = markers[1], markersize = 15), \n",
    "                   mlines.Line2D([0], [0], color=colors[2], lw = 5, label = 'DiSC-UCB2', marker = markers[2], markersize = 15)]\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('round t', fontsize=25)\n",
    "plt.ylabel(r'average cumulative regret $\\frac{Rt}{t}$', fontsize=25)\n",
    "plt.title('synthetic data', fontsize=25)\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d25f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
